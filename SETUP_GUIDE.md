# üìã Guide de Configuration - Torrent Dashboard

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MACHINE DISTANTE  ‚îÇ
‚îÇ   (Windows/Linux)   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ scraper.py    ‚îÇ  ‚îÇ Tourne 24h/24
‚îÇ  ‚îÇ + .env        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ FTP Upload
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  O2SWITCH (Server)  ‚îÇ
‚îÇ /public_html/dash/  ‚îÇ
‚îÇ  - stats.json       ‚îÇ
‚îÇ  - history.json     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ HTTP GET
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Next.js Dashboard   ‚îÇ
‚îÇ (dash.example) ‚îÇ
‚îÇ /api/stats ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ /api/history ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÄ Proxy vers server
‚îÇ                  ‚îÇ  
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº Affichage
      Frontend UI
```

---

## 1Ô∏è‚É£ Configuration de la MACHINE DISTANTE (avec scraper)

### √âtape 1: Installer Python + d√©pendances

```bash
# Windows
python -m pip install -r requirements.txt
python -m playwright install chromium

# Linux
python3 -m pip install -r requirements.txt
python3 -m playwright install chromium
```

### √âtape 2: Cr√©er le fichier `.env`

√Ä la **racine du projet** (m√™me dossier que `scraper.py`):

```env
# FTP O2Switch
FTP_HOST=pin.o2switch.net
FTP_USER=sana6906
FTP_PASS=58mD-fXqY-AEJ)
FTP_DIR=/public_html/dash

# Generation-Free (UNIT3D)
GF_USER=REDACTED_USER
GF_PASS=REDACTED_PASSWORD

# TheOldSchool (UNIT3D)
TOS_USER=REDACTED_USER
TOS_PASS=REDACTED_PASSWORD

# Sharewood
SW_USER=REDACTED_USER
SW_PASS=Nat#301101
```

### √âtape 3: Tester le scraper

```bash
# Test une ex√©cution
python scraper.py

# Doit cr√©er:
# - stats.json (donn√©es actuelles)
# - history.json (historique)
# - Upload via FTP
```

### √âtape 4: Programmer l'ex√©cution 24h/24

#### ‚è∞ Windows (Task Scheduler)

1. Ouvrir "Planificateur de t√¢ches"
2. Cr√©er une t√¢che basique
   - **Nom:** `TorrentScraper`
   - **D√©clencheur:** Quotidien, toutes les 6h
   - **Action:** 
     ```
     Program: C:\Python312\python.exe
     Arguments: C:\chemin\vers\scraper.py
     ```

#### üêß Linux (Crontab)

```bash
# √âditer crontab
crontab -e

# Ajouter (chaque 6 heures):
0 */6 * * * cd /home/user/dashboard && python3 scraper.py >> scraper.log 2>&1

# Ou chaque heure:
0 * * * * cd /home/user/dashboard && python3 scraper.py >> scraper.log 2>&1
```

---

## 2Ô∏è‚É£ Configuration du SERVEUR WEB (O2Switch)

### V√©rifier l'acc√®s aux fichiers

Les fichiers doivent √™tre **accessibles en HTTP**:

- `https://dash.example.com/stats.json` ‚úÖ
- `https://dash.example.com/history.json` ‚úÖ

Si NON accessible:
1. V√©rifier les permissions FTP
2. V√©rifier que le dossier `/public_html/dash/` est **public**

### Optionnel: Compresser les fichiers

Si l'historique devient trop volumineux:

```bash
# Sur le serveur, compresser les anciens fichiers
gzip history.json
# Renommer en history.json.gz
```

---

## 3Ô∏è‚É£ D√©ployer le SITE NEXT.JS

### Options de d√©ploiement:

#### Option A: Vercel (Recommand√© - gratuit)
```bash
npm install -g vercel
vercel
# Puis configurer le domaine dash.example.com
```

#### Option B: O2Switch (Shared Hosting)
```bash
# Build
cd torrent-dashboard
npm run build
npm run start

# Sur O2Switch, utiliser Node.js si disponible
# Ou d√©ployer via FTP les fichiers statiques de .next/
```

#### Option C: VPS Personnel
```bash
# SSH vers ton serveur
ssh user@dash.example.com

# Cloner et d√©ployer
git clone https://github.com/BurN-30/dashboard-ratio.git
cd dashboard-ratio/torrent-dashboard
npm install
npm run build
pm2 start npm --name "dashboard" -- start
```

---

## 4Ô∏è‚É£ Variables d'Environnement du Site

Cr√©er `.env.local` dans `torrent-dashboard/`:

```env
# Pas n√©cessaire pour le moment (API en interne)
# Les identifiants du scraper REST S√âCURIS√âS sur la machine distante
```

---

## ‚úÖ Checklist de V√©rification

- [ ] Scraper fonctionne en local: `python scraper.py`
- [ ] Fichiers JSON g√©n√©r√©s: `stats.json`, `history.json`
- [ ] FTP upload r√©ussit
- [ ] `https://dash.example.com/stats.json` accessible
- [ ] `https://dash.example.com/history.json` accessible
- [ ] Site Next.js d√©ploy√©: `https://dash.example.com/`
- [ ] API proxy fonctionne: `https://dash.example.com/api/stats`
- [ ] Dashboard affiche les donn√©es
- [ ] Scraper planifi√© 24h/24

---

## üîí S√©curit√© - √Ä am√©liorer

‚ö†Ô∏è **AVANT PRODUCTION:**

1. **Mettre les identifiants en variables d'environnement** ‚úÖ (d√©j√† fait)
2. **Prot√©ger les fichiers JSON** - Ajouter une authentification HTTP
3. **Utiliser HTTPS partout** ‚úÖ
4. **Limiter l'acc√®s aux fichiers JSON** au bot scraper uniquement

### Am√©lioration: Prot√©ger les JSON avec mot de passe

```python
# scraper.py - Ajouter l'authentification
FTP_PASS_HASH = os.getenv("FTP_PASS_HASH")  # G√©n√©rer avec htpasswd

# O2Switch .htaccess
<Files "*.json">
    AuthType Basic
    AuthName "Stats"
    AuthUserFile /path/to/.htpasswd
    Require valid-user
</Files>
```

---

## üìû Troubleshooting

| Probl√®me | Solution |
|----------|----------|
| `playwright not found` | `python -m playwright install chromium` |
| FTP timeout | V√©rifier l'IP, firewall |
| Fichiers JSON non √† jour | V√©rifier le cron/Task Scheduler |
| API retourne 404 | V√©rifier `https://dash.example.com/stats.json` |
| Dashboard ne charge pas | V√©rifier la console (F12), erreurs r√©seau |

