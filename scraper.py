import json
import time
import os
import re
from datetime import datetime
from playwright.sync_api import sync_playwright
from dotenv import load_dotenv

# --- 1. FORCER LE DOSSIER DE TRAVAIL (Indispensable pour l'automatisation) ---
# Cela permet au script de trouver les JSON m√™me s'il est lanc√© par Windows
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)

# Charger les variables d'environnement depuis .env
load_dotenv()

# --- CONFIGURATION FICHIER ---
OUTPUT_FILE = "stats.json"
HISTORY_FILE = "history.json"

# --- CONFIGURATION SITES ---
SITES = [
    {
        "name": "Generation-Free",
        "login_url": "https://generation-free.org/login",
        "profile_url": "https://generation-free.org/users/REDACTED_USER", 
        "username": os.getenv("GF_USER"),
        "password": os.getenv("GF_PASS"),
        "type": "unit3d"
    },
    {
        "name": "TheOldSchool",
        "login_url": "https://theoldschool.cc/login",
        "profile_url": "https://theoldschool.cc/users/REDACTED_USER", 
        "username": os.getenv("TOS_USER"),
        "password": os.getenv("TOS_PASS"),
        "type": "unit3d"
    },
    {
        "name": "Sharewood",
        "login_url": "https://sharewood.tv/login",
        "profile_url": "https://www.sharewood.tv/REDACTED_USER.97438",
        "username": os.getenv("SW_USER"),
        "password": os.getenv("SW_PASS"),
        "type": "sharewood"
    }
]

def clean(text):
    """Nettoie le texte (espaces, retours √† la ligne) et convertit les unit√©s"""
    if not text: return "0"
    text = text.replace('\xa0', ' ').replace('\u202f', ' ').strip().replace('\n', ' ')
    
    # Conversion des unit√©s (Anglais -> Fran√ßais)
    text = text.replace("TiB", "To")
    text = text.replace("GiB", "Go")
    text = text.replace("MiB", "Mo")
    text = text.replace("KiB", "Ko")
    
    # Remplacement de " B" par " o" (seulement si √† la fin pour √©viter les faux positifs)
    if text.endswith(" B"):
        text = text[:-1] + "o"
        
    return text

def format_duration(text):
    """Formate la dur√©e en fran√ßais et standardise l'affichage"""
    if not text or text == "0": return "0"
    
    # 1. Ajouter des espaces entre les chiffres et les lettres si absents (ex: 1M2S -> 1M 2S)
    text = re.sub(r'(\d+)([a-zA-Z]+)', r'\1 \2', text)
    
    # 1b. Ajouter des espaces entre les lettres et les chiffres (ex: M2 -> M 2)
    text = re.sub(r'([a-zA-Z]+)(\d+)', r'\1 \2', text)
    
    # 2. Nettoyer les espaces multiples
    text = text.replace("  ", " ").strip()
    
    # 3. Remplacements d'unit√©s
    # Ann√©es
    text = re.sub(r'\b(\d+)\s*[Yy]\b', r'\1 an(s)', text)
    
    # Mois (M majuscule)
    text = re.sub(r'\b(\d+)\s*M\b', r'\1 mois', text)
    
    # Semaines (W ou S majuscule)
    text = re.sub(r'\b(\d+)\s*[Ww]\b', r'\1 sem', text)
    text = re.sub(r'\b(\d+)\s*S\b', r'\1 sem', text)
    
    # Jours
    text = re.sub(r'\b(\d+)\s*[Dd]\b', r'\1 j', text)
    text = re.sub(r'\b(\d+)\s*[Jj]\b', r'\1 j', text)
    
    # Heures
    text = re.sub(r'\b(\d+)\s*[Hh]\b', r'\1 h', text)
    
    # Minutes (m minuscule)
    text = re.sub(r'\b(\d+)\s*m\b', r'\1 min', text)
    
    # Secondes (s minuscule)
    text = re.sub(r'\b(\d+)\s*s\b', r'\1 s', text)
    
    return text

def get_unit3d_val(page, label, exact=False):
    """Cherche une valeur dans les listes key-value de UNIT3D"""
    # Use normalize-space to handle newlines and extra spaces in labels
    if exact:
        xpath = f'//dt[normalize-space(.)="{label}"]/following-sibling::dd'
    else:
        xpath = f'//dt[contains(normalize-space(.), "{label}")]/following-sibling::dd'
    try:
        # On laisse 1 seconde pour trouver l'√©l√©ment (suffisant si la page est charg√©e)
        # au lieu de 30s par d√©faut qui bloquait tout si le champ √©tait absent
        return clean(page.locator(xpath).first.inner_text(timeout=1000))
    except:
        return "0"

def scrape_sharewood(page):
    data = {}
    try:
        # 1. BARRE DU HAUT
        data['count_download'] = clean(page.inner_text('//span[contains(., "Total downloads")]')).split(':')[-1].strip()
        data['count_seed'] = clean(page.inner_text('//span[contains(., "Total en seed")]')).split(':')[-1].strip()
        data['count_leech'] = clean(page.inner_text('//span[contains(., "Total en leech")]')).split(':')[-1].strip()

        # 2. TABLEAU PRINCIPAL
        data['vol_download'] = clean(page.inner_text('span[data-original-title="T√©l√©charg√©"]'))
        
        try:
            up_total = clean(page.inner_text('span[data-original-title="Upload enregistr√©"]'))
            data['vol_upload'] = up_total 
        except:
            data['vol_upload'] = clean(page.inner_text('//tr[td[strong[contains(text(), "Upload")]]]/td[2]'))

        data['ratio'] = clean(page.inner_text('//tr[td[strong[contains(text(), "Ratio")]]]/td[2]'))
        data['buffer'] = clean(page.inner_text('//tr[td[strong[contains(text(), "Capacit√© de DL")]]]/td[2]'))
        data['time_seed_total'] = format_duration(clean(page.inner_text('//tr[td[strong[contains(text(), "Temps total de seed")]]]/td[2]')))
        data['time_seed_avg'] = format_duration(clean(page.inner_text('//tr[td[strong[contains(text(), "Temps de seed moyen")]]]/td[2]')))

        # 3. SECTION EXTRA
        data['points_bonus'] = clean(page.inner_text('//strong[contains(., "Bonus")]/following-sibling::span'))
        data['fl_tokens'] = clean(page.inner_text('//strong[contains(., "Jetons FL")]/following-sibling::span'))
        data['hit_and_run'] = clean(page.inner_text('//strong[contains(., "Hit&Run")]/following-sibling::span'))
        
        # Warnings (Sharewood)
        try:
            # "Warnings Avertissements en cours : 0 / 3"
            warnings_text = clean(page.inner_text('//span[contains(., "Avertissements en cours")]'))
            # Extraction de "0 / 3"
            parts = warnings_text.split(":")[-1].strip().split("/")
            data['warnings_active'] = parts[0].strip()
            data['warnings_limit'] = parts[1].strip() if len(parts) > 1 else "3"
        except:
            data['warnings_active'] = "0"
            data['warnings_limit'] = "3"

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur partielle Sharewood: {e}")
    
    return data

def scrape_unit3d(page, site_name):
    data = {}
    
    # AVERTISSEMENTS
    data['warnings_active'] = get_unit3d_val(page, "Avertissements actifs")
    data['hit_and_run'] = get_unit3d_val(page, "Hit and Run Count")
    if data['hit_and_run'] == "0":
         data['hit_and_run'] = get_unit3d_val(page, "Compteur de Hit and Run")
    
    # SEED STATS
    data['seed_time_total'] = format_duration(get_unit3d_val(page, "Dur√©e totale des seeds"))
    if data['seed_time_total'] == "0": 
        data['seed_time_total'] = format_duration(get_unit3d_val(page, "Temps de seed total"))
        
    data['seed_time_avg'] = format_duration(get_unit3d_val(page, "Temps de seed moyen"))
    data['seed_size'] = get_unit3d_val(page, "Seeding Size")
    if data['seed_size'] == "0": data['seed_size'] = get_unit3d_val(page, "Volume de Seed")

    # TORRENT COUNT
    data['count_downloaded'] = get_unit3d_val(page, "Total des t√©l√©charg√©s")
    if data['count_downloaded'] == "0": data['count_downloaded'] = get_unit3d_val(page, "Total compl√©t√©s")

    data['count_seed'] = get_unit3d_val(page, "Total en seed")
    data['count_leech'] = get_unit3d_val(page, "Total en leech")

    # TRAFFIC
    data['ratio'] = get_unit3d_val(page, "Ratio")
    data['real_ratio'] = get_unit3d_val(page, "Real Ratio")
    data['buffer'] = get_unit3d_val(page, "Tampon")
    
    data['vol_upload'] = get_unit3d_val(page, "Compte Envoyer (Total)")
    if data['vol_upload'] == "0": data['vol_upload'] = get_unit3d_val(page, "Compte Uploader (Total)")

    data['vol_download'] = get_unit3d_val(page, "Compte T√©l√©charger (Total)")
    if data['vol_download'] == "0": data['vol_download'] = get_unit3d_val(page, "Compte T√©l√©charger")

    # TORRENT TRAFFIC DETAILS
    data['torrent_uploader'] = get_unit3d_val(page, "Torrent Envoyer")
    if data['torrent_uploader'] == "0": data['torrent_uploader'] = get_unit3d_val(page, "Torrent Uploader")
    
    data['torrent_uploader_credited'] = get_unit3d_val(page, "Torrent Envoyer (Credit√©)")
    if data['torrent_uploader_credited'] == "0": data['torrent_uploader_credited'] = get_unit3d_val(page, "Torrent Uploader (Credit√©)")

    data['torrent_downloader'] = get_unit3d_val(page, "Torrent T√©l√©charger")

    # RECOMPENSES / POINTS
    # Normalisation: on utilise 'points_bonus' pour tout le monde
    data['points_bonus'] = get_unit3d_val(page, "Coupon", exact=True)
    if data['points_bonus'] == "0" or data['points_bonus'] == "":
         data['points_bonus'] = get_unit3d_val(page, "Point Bonus", exact=True)
    
    data['fl_tokens'] = get_unit3d_val(page, "Jetons Freeleech")
    # data['thanks_given'] = get_unit3d_val(page, "Merci donn√©")
    # data['thanks_received'] = get_unit3d_val(page, "Merci re√ßu")
    
    # COMMUNITY / EXTRA
    # data['invitations'] = get_unit3d_val(page, "Invitations")
    # data['points_bonus_received'] = get_unit3d_val(page, "Points Bonus re√ßus")
    
    return data

def clean_history(history):
    """
    Optimise l'historique pour ne pas qu'il grossisse ind√©finiment.
    - Moins de 30 jours : On garde TOUTES les entr√©es.
    - Plus de 30 jours : On ne garde qu'UNE entr√©e par jour.
    """
    if not history: return []

    now = time.time()
    thirty_days_ago = now - (30 * 24 * 3600)
    
    new_history = []
    seen_days = set()
    
    # On parcourt l'historique (du plus vieux au plus r√©cent g√©n√©ralement)
    for entry in history:
        ts = entry.get('_timestamp')
        if not ts: continue
        
        if ts > thirty_days_ago:
            # C'est r√©cent (moins de 30j) : on garde tout
            new_history.append(entry)
        else:
            # C'est vieux : on ne garde qu'une entr√©e par jour
            date_str = datetime.fromtimestamp(ts).strftime('%Y-%m-%d')
            if date_str not in seen_days:
                new_history.append(entry)
                seen_days.add(date_str)
    
    return new_history

def main():
    final_data = {}

    # 1. SCRAPING
    print("--- üï∑Ô∏è D√©marrage du scraping ---")
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        for site in SITES:
            print(f"-> Traitement de : {site['name']}")
            try:
                page.goto(site['login_url'])
                try:
                    if page.locator('input[name="username"]').count() > 0:
                        if not site['username'] or not site['password']:
                            print(f"   ‚ö†Ô∏è  Identifiants manquants pour {site['name']} (V√©rifiez le fichier .env)")
                        else:
                            page.fill('input[name="username"]', site['username'])
                            page.fill('input[name="password"]', site['password'])
                            page.press('input[name="password"]', 'Enter')
                            page.wait_for_load_state('networkidle')
                except Exception as login_err:
                    print(f"   Info Login: {login_err}")
                
                page.goto(site['profile_url'])
                time.sleep(2) 

                if site['type'] == 'sharewood':
                    final_data[site['name']] = scrape_sharewood(page)
                else:
                    final_data[site['name']] = scrape_unit3d(page, site['name'])
                
                print(f"   ‚úÖ OK")

            except Exception as e:
                print(f"   ‚ùå Erreur: {e}")

        browser.close()

    # 2. SAUVEGARDE LOCALE
    # Ajout du timestamp
    final_data["_timestamp"] = int(time.time())

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, indent=4, ensure_ascii=False)
    print(f"\nüíæ Fichier {OUTPUT_FILE} g√©n√©r√© en local.")

    # Gestion de l'historique
    history = []
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except:
            print("‚ö†Ô∏è Erreur lecture historique, cr√©ation d'un nouveau.")
    
    history.append(final_data)
    
    # Nettoyage intelligent de l'historique
    history = clean_history(history)

    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=4, ensure_ascii=False)
    print(f"üíæ Fichier {HISTORY_FILE} mis √† jour ({len(history)} entr√©es).")

if __name__ == "__main__":
    main()